{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZKr5-cJ92NL"
   },
   "source": [
    "# Manufacturing Causal-Net \n",
    "\n",
    "The target of the notebook is to learn *causal relations* in a dataset generated from a manufacturing simulator and, secondly, apply some *do-calculus operation* to observe effects of potential intervetions.\n",
    "\n",
    "The simulator has been built using *Simpy*. Furhter information about Simpy in the following link: https://simpy.readthedocs.io/en/latest/index.html\n",
    "\n",
    "The tools used are: \n",
    "- Pandas for data import and manipulation\n",
    "- CausalNex for causal learning\n",
    "\n",
    "In particular, \"*CausalNex is a Python library that uses Bayesian Networks to combine machine learning and domain expertise for causal reasoning*\". \n",
    "\n",
    "The relative documentation is available here: https://causalnex.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AtSHRXxKcwC"
   },
   "source": [
    "## Preparing the workspace\n",
    "All the necessary tools has been downloaded and installed via the venv associated to the project. \n",
    "Refer to the requirements.txt file for further information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SulWm8sBKwKA"
   },
   "source": [
    "### Importing necessary libraries\n",
    "All the other necessary lirbaries are imported. They are: \n",
    "- os, time, datetime: for file handling operations \n",
    "- pandas: for data import, cleaning and input for CausalNex \n",
    "- causalnex.structure.notears: for network generation from pandas data\n",
    "- networkx: for resulting network plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "v27BQ35ognvn"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import networkx\n",
    "import pandas\n",
    "from causalnex.structure.notears import from_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7AnRBdQMwxX"
   },
   "source": [
    "## Preparing the data set\n",
    "The dataset generated from the simulation is exported as within a folder. \n",
    "\n",
    "The folder name is \"yyyy.mm.dd-hh.mm-log\", reporting the moment in time where the simulation started. The folder is placed inside of the Google Colab project folder.\n",
    "\n",
    "The file containing the data is called \"merged_logs.csv\". It is imported as a Pandas Dataframe with the method \"read_csv\".\n",
    "\n",
    "After the import, the head of the dataset and other dataset features are displayed.\n",
    "\n",
    "---\n",
    "\n",
    "The dataset used for the published experiment is \"2022.03.14-11.54-log\".\n",
    "\n",
    "CORRECT THE LAST ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the last zip log file\n",
    "# Directory list \n",
    "dir = os.listdir('..\\manufacturing_model\\logs')\n",
    "\n",
    "el = 'none'\n",
    "comp = 'none'\n",
    "for i in range(len(dir)):\n",
    "    if dir[i].endswith('.zip'):\n",
    "        el = dir[i]\n",
    "    \n",
    "    if i > 0 and dir[i] > el and dir[i].endswith('.zip'): \n",
    "            el = dir[i]\n",
    "\n",
    "# Selected folder\n",
    "working_folder = el.replace('.zip', '')\n",
    "zip_dataset_file = el\n",
    "# Creating the folder to store the last zip file\n",
    "os.mkdir('dataset\\\\' + working_folder)\n",
    "# Moving the file\n",
    "shutil.copyfile(src='..\\manufacturing_model\\logs\\\\' + zip_dataset_file, dst='dataset\\\\' + working_folder  + '\\\\' + zip_dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "2T8KD3q2_ORk"
   },
   "outputs": [],
   "source": [
    "# Setting the csv path \n",
    "CSV_PATH = working_folder \n",
    "\n",
    "CSV_PATH = 'dataset'\n",
    "\n",
    "CSV_FILE_NAME = '/merged_logs.csv'\n",
    "CSV_FILE_PATH = CSV_PATH + CSV_FILE_NAME\n",
    "\n",
    "LIGHT_CSV_FILE_NAME = '/light-logs.csv'\n",
    "LIGHT_CSV_FILE_PATH = CSV_PATH + LIGHT_CSV_FILE_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS69MJvzGgEI",
    "outputId": "d14df7f2-8507-413b-bc78-c4732f082ca8"
   },
   "outputs": [
    {
     "ename": "ReadError",
     "evalue": "2022.09.12-17.51.zip is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [68], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preparing the data set\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Unzipping the folder\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_dataset_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Getting the dataframe from the file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_csv(CSV_FILE_PATH, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py:1221\u001b[0m, in \u001b[0;36munpack_archive\u001b[1;34m(filename, extract_dir, format)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown unpack format \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1220\u001b[0m     func \u001b[38;5;241m=\u001b[39m format_info[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1221\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mformat_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m     \u001b[38;5;66;03m# we need to look at the registered unpackers supported extensions\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m _find_unpack_format(filename)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py:1125\u001b[0m, in \u001b[0;36m_unpack_zipfile\u001b[1;34m(filename, extract_dir)\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m  \u001b[38;5;66;03m# late import for breaking circular dependency\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mis_zipfile(filename):\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28mzip\u001b[39m \u001b[38;5;241m=\u001b[39m zipfile\u001b[38;5;241m.\u001b[39mZipFile(filename)\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mReadError\u001b[0m: 2022.09.12-17.51.zip is not a zip file"
     ]
    }
   ],
   "source": [
    "# Preparing the data set\n",
    "# Unzipping the folder\n",
    "shutil.unpack_archive(zip_dataset_file, format='zip')\n",
    "\n",
    "# Getting the dataframe from the file\n",
    "data = pandas.read_csv(CSV_FILE_PATH, delimiter=',')\n",
    "\n",
    "# Displaying the head and other dataset characteristics\n",
    "print(data.head(10))\n",
    "print('\\n')\n",
    "print(data.dtypes)\n",
    "print('\\n')\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaLOprUPYe_9"
   },
   "source": [
    "## Data cleaning and preparation\n",
    "Since the dataset is virtually generated, NaN or missing value are not present.\n",
    "\n",
    "Btw, some data preparation is computed. \n",
    "\n",
    "### Splitting the step column\n",
    "The step column has the form of \"step.moment\": this is not really the meaning of the step used by Simpy. Is instead a \"trick\" to make logs and debugging easier in the previous phase.\n",
    "\n",
    "So, for this phase, is more coherent to split this column into two different columns, namely \"step\" and \"moment\".\n",
    "\n",
    "---\n",
    "\n",
    "The split is performed in 3 steps: \n",
    "1. The \"step\" column is converted in type, from float to string. The result is saved in a new columns called \"step_str\" attached on the right to the initial dataframe. \n",
    "2. Using the str.split method, the \"step_str\" column is split into 2 columns at the \".\" (*point*). The resulting columns are saved into 2 columns called \"step\" and \"moment\". \n",
    "3. The no more necessary temporary column \"step_str\" is dropped.\n",
    "\n",
    "Finally, the columns are reordered keeping the new \"step\" and \"moment\" columns on the left of the dataset and converting them into int type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-d1Ug0CA8jAA",
    "outputId": "5f294550-e7d4-4243-a27c-6a776192e0ea"
   },
   "outputs": [],
   "source": [
    "# Splitting the \"step\" column into \"step\" and \"moment\" in 3 steps:\n",
    "# 1. Converting the step col into string type\n",
    "data[\"step_str\"] = data[\"step\"].astype(str)\n",
    "# 2. Using str.split to split the col at the \".\"\n",
    "data[[\"step\", \"moment\"]] = data.step_str.str.split(\".\", expand = True)\n",
    "# 3. Dropping the temp col\n",
    "data.drop(columns=[\"step_str\"], inplace=True)\n",
    "\n",
    "# Reordering the result\n",
    "data = data[[\"step\", \"moment\", \"failure Machine A\", \"Machine A flag\", \n",
    "      \"failure Machine B\",  \"Machine B flag\", \"failure Machine C\", \n",
    "      \"Machine C flag\"]]\n",
    "\n",
    "# Converting everything in int \n",
    "data = data.astype(int)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWxQjhAKahEv"
   },
   "source": [
    "## Dropping unnecessary columns for learning \n",
    "In the previous step, during the columns reordering, not all the columns of the initial dataset have been used. For that reason, the unused columns have been dropped. \n",
    "\n",
    "So, there are only 2 columns left to be dropped: \"step\" and \"moment\". \n",
    "\n",
    "They are not necessary since the Bayesian Network used are not time-dependent: they just analyse the datast line-to-line trying to understand relations between features, without taking into account the time. \n",
    "\n",
    "In the following, \"step\" and \"moment\" are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHQYEOymRd7t",
    "outputId": "9cf12070-c39c-4d93-f047-9cad347efaf0"
   },
   "outputs": [],
   "source": [
    "# Dropping the unneccessary columns\n",
    "data.drop(columns=[\"step\", \"moment\"], inplace=True)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma7RPJBURQ_x"
   },
   "source": [
    "## Saving the cleaned dataset in a dedicated CSV file\n",
    "The processed dataset is now ready in order to be processed by CausalNex. \n",
    "\n",
    "Before launching the learning phase, the dataset is exported as a CSV file in the same file location of the initial dataset CSV. \n",
    "\n",
    "The file is called \"light-logs.csv\", because \"lighter\" with respect to the initial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSbLiXfwpFEP"
   },
   "outputs": [],
   "source": [
    "# Saving the light dataset into csv\n",
    "data.to_csv(LIGHT_CSV_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTWRoxMISE1o"
   },
   "source": [
    "## Data analysis \n",
    "Before the learning phase, data is analysed in order to understand the impact of the dataset structure on the causal-learning. \n",
    "\n",
    "The structure of the dataset, in fact, will let us understand in advance if some relationships will be caught or not. \n",
    "\n",
    "To do so, the following metrics will be computed: \n",
    "- fault_Machine x = 1 AND flag_Machine_x = 1 AND flag_Machine_C = 1\n",
    "- fault_Machine x = 1 AND flag_Machine_x = 1 AND flag_Machine_C = 0\n",
    "\n",
    "The ratio between those 2 metrics will let the user understand if the breakdown of an upstream machine (Machine A and B in the sim) has affected the downstream machine (Machine C in the sim). \n",
    "\n",
    "If the impact is high, the relation is likely to be caught. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "vnBkDMri-J_X",
    "outputId": "21db205c-8d18-4437-af2c-cfd49acf6e82"
   },
   "outputs": [],
   "source": [
    "# Loading the light dataset into csv\n",
    "data = pandas.read_csv(LIGHT_CSV_FILE_PATH, delimiter=',', index_col=0)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUBC7tVYUZjo",
    "outputId": "0c76ec8e-c674-4634-912f-02d3f39a5e79"
   },
   "outputs": [],
   "source": [
    "# Machine A Ratio\n",
    "fault_A_flag = data[(data['failure Machine A'] == 1) & \n",
    "                    (data['Machine A flag'] == 1)].count()\n",
    "\n",
    "fault_A_flag_C_High = data[(data['failure Machine A'] == 1) & \n",
    "                           (data['Machine A flag'] == 1) & \n",
    "                           (data['Machine C flag'] == 1)].count()\n",
    "\n",
    "fault_A_flag_C_Low = data[(data['failure Machine A'] == 1) & \n",
    "                          (data['Machine A flag'] == 1) & \n",
    "                          (data['Machine C flag'] == 0)].count()\n",
    "\n",
    "ratio_A_high = fault_A_flag_C_High[0]/fault_A_flag[0]\n",
    "ratio_A_low = fault_A_flag_C_Low[0]/fault_A_flag[0]\n",
    "\n",
    "print('Ratio A High: ', ratio_A_high)\n",
    "print('Ratio A Low: ', ratio_A_low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WgIaxPZY6lV",
    "outputId": "e2a5ddba-3f66-4724-b1e6-6b3d64da320f"
   },
   "outputs": [],
   "source": [
    "# Machine B Ratio\n",
    "fault_B_flag = data[(data['failure Machine B'] == 1) & \n",
    "                    (data['Machine B flag'] == 1)].count()\n",
    "\n",
    "fault_B_flag_C_High = data[(data['failure Machine B'] == 1) & \n",
    "                           (data['Machine B flag'] == 1) & \n",
    "                           (data['Machine C flag'] == 1)].count()\n",
    "\n",
    "fault_B_flag_C_Low = data[(data['failure Machine B'] == 1) & \n",
    "                          (data['Machine B flag'] == 1) & \n",
    "                          (data['Machine C flag'] == 0)].count()\n",
    "\n",
    "ratio_B_high = fault_B_flag_C_High[0]/fault_B_flag[0]\n",
    "ratio_B_low = fault_B_flag_C_Low[0]/fault_B_flag[0]\n",
    "\n",
    "print('Ratio B High: ', ratio_B_high)\n",
    "print('Ratio B Low: ', ratio_B_low)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSJMy_pAR-DE"
   },
   "source": [
    "## CausalNex application: applying the causal-network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sLr0Z9pmkvf",
    "outputId": "4e6ec2d2-79ff-4ed9-edc7-dffa9797f727"
   },
   "outputs": [],
   "source": [
    "# Eliminating spaces from column names - useful for later\n",
    "column_dict = {}\n",
    "for el in data.columns:\n",
    "  key = el\n",
    "  el = el.replace(' ', '_')\n",
    "  column_dict [key] = el\n",
    "\n",
    "data = data.rename(columns=column_dict)\n",
    "\n",
    "# Getting tabu child nodes\n",
    "tabu_child_list = [x for x in data.columns if 'failure' in x]\n",
    "\n",
    "print('Tabu child list: ', tabu_child_list)\n",
    "\n",
    "# Training the model\n",
    "# Declaring and mining the structure of the causal-net\n",
    "start_time = time.time()\n",
    "structure_model = from_pandas(data, tabu_child_nodes=tabu_child_list)\n",
    "finish_time = time.time()\n",
    "sim_time = finish_time - start_time\n",
    "\n",
    "# Printing the structure model with Python\n",
    "print(\"Total training time: {} min, {} secs\".format(round(sim_time/60, 0), \n",
    "                                                    round(sim_time%60, 0)))\n",
    "# print(\"Total training time: {} seconds\".format(round(sim_time, 2)/60))\n",
    "print(structure_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxjraST7GRj9",
    "outputId": "bf3d0c54-518e-424b-d894-736b101c5021"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "N2qwPglboYKw",
    "outputId": "48dd420d-33cd-47f1-da3e-49936885a03c"
   },
   "outputs": [],
   "source": [
    "# Printing the structure model with networkx\n",
    "networkx.draw(structure_model, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "yWlktXteDH7S",
    "outputId": "c2c25cba-6103-4955-9262-e2f7df9b4f4d"
   },
   "outputs": [],
   "source": [
    "structure_model.remove_edges_below_threshold(0.1)\n",
    "print(structure_model)\n",
    "# Printing the structure model again\n",
    "networkx.draw(structure_model, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "s_iO3mnRIXid",
    "outputId": "d694140d-408a-4f5e-a889-978739957fab"
   },
   "outputs": [],
   "source": [
    "structure_model.remove_edges_below_threshold(0.2)\n",
    "print(structure_model)\n",
    "# Printing the structure model again\n",
    "networkx.draw(structure_model, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "5BNerdjwIZU4",
    "outputId": "6d18f1ee-6061-40d8-dbb8-263114fe3e35"
   },
   "outputs": [],
   "source": [
    "structure_model.remove_edges_below_threshold(0.3)\n",
    "print(structure_model)\n",
    "# Printing the structure model again\n",
    "networkx.draw(structure_model, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIQ6BESQosOI"
   },
   "outputs": [],
   "source": [
    "# Exporting the model at the best threshold (as said in the NOTEARS arxiv paper)\n",
    "networkx.drawing.nx_pydot.write_dot(structure_model, CSV_PATH + '/graph.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Fh0XpKKBMR1"
   },
   "outputs": [],
   "source": [
    "# Getting weights of the generated model in Python dict structure\n",
    "edges_weights_dict = networkx.to_dict_of_dicts(structure_model)\n",
    "\n",
    "# Saving the weights of the generated model\n",
    "with open(CSV_PATH + '/edges-weights.txt', 'a') as f:\n",
    "  for k1 in edges_weights_dict:\n",
    "    for k2 in edges_weights_dict[k1]:\n",
    "      text = k1 + ' -> ' + k2 + ': ' \\\n",
    "            + str(edges_weights_dict[k1][k2].get('weight'))\n",
    "    f.write(text)\n",
    "  f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31pP03ki6EFJ",
    "outputId": "0c2eb95e-20d8-412e-b088-aaa2718609d9"
   },
   "outputs": [],
   "source": [
    "# Bayesian network instantiation\n",
    "from causalnex.network import BayesianNetwork\n",
    "\n",
    "bayesian_net = BayesianNetwork(structure_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tagZPOgMygNe",
    "outputId": "bf933f8c-d3d0-4e91-92e1-ad642ad8e77e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 90% train and 10% test\n",
    "training_data = data.copy()\n",
    "train, test = train_test_split(training_data, train_size=0.9, test_size=0.1, \n",
    "                               random_state=7)\n",
    "\n",
    "# Fitting node states into the Bayesian Network: here they are inferred from the \n",
    "# input data, but sometimes is necessary to provide a dictionary for them to be \n",
    "# assigned\n",
    "bayesian_net = bayesian_net.fit_node_states(training_data)\n",
    "\n",
    "# Fitting the data into the prepared net\n",
    "bayesian_net = bayesian_net.fit_cpds(train, method=\"BayesianEstimator\", \n",
    "                                     bayes_prior=\"K2\")\n",
    "\n",
    "bayesian_net.cpds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "7K6ShCgkJJIl",
    "outputId": "2e7c8b35-acc9-48a9-a169-4a4547941fbd"
   },
   "outputs": [],
   "source": [
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MwfQlcUJQAx",
    "outputId": "72f57ed8-8e92-4d39-b6f7-ad4f7e82ff8a"
   },
   "outputs": [],
   "source": [
    "ser = test.groupby([\"failure_Machine_A\"]).size()\n",
    "ser \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8l_7Vgtg8iJi",
    "outputId": "be691c2d-bf5e-4fdf-c5cc-a1fd08bbdf4a"
   },
   "outputs": [],
   "source": [
    "from causalnex.evaluation import classification_report, roc_auc\n",
    "\n",
    "# Model quality evaluation for failure Machine A\n",
    "# classification_report(bayesian_net, test, \"failure_Machine_A\")\n",
    "\n",
    "# ROC, AUC curve computation for failure Machine A\n",
    "machine_A_failure_roc, machine_A_failure_auc = roc_auc(bayesian_net, test, \"failure_Machine_A\")\n",
    "#print('Machine A failure Node ROC: ', machine_A_failure_roc)\n",
    "print('Machine A failure Node AUC: ', machine_A_failure_auc)\n",
    "\n",
    "# Model quality evaluation for Machine A flag\n",
    "# classification_report(bayesian_net, test, \"Machine_A_flag\")\n",
    "\n",
    "# ROC, AUC curve computation for Machine A flag\n",
    "machine_A_flag_roc, machine_A_flag_auc = roc_auc(bayesian_net, test, \"Machine_A_flag\")\n",
    "#print('Machine A flag Node ROC: ', machine_A_flag_roc)\n",
    "print('Machine A flag Node AUC: ', machine_A_flag_auc)\n",
    "\n",
    "# Model quality evaluation for failure Machine B\n",
    "# classification_report(bayesian_net, test, \"failure_Machine_B\")\n",
    "\n",
    "# ROC, AUC curve computation for failure Machine B\n",
    "machine_B_failure_roc, machine_B_failure_auc = roc_auc(bayesian_net, test, \"failure_Machine_B\")\n",
    "#print('Machine B failure Node ROC: ', machine_B_failure_roc)\n",
    "print('Machine B failure Node AUC: ', machine_B_failure_auc)\n",
    "\n",
    "# Model quality evaluation for Machine B flag\n",
    "# classification_report(bayesian_net, test, \"Machine_B_flag\")\n",
    "\n",
    "# ROC, AUC curve computation for Machine B flag\n",
    "machine_B_flag_roc, machine_B_flag_auc = roc_auc(bayesian_net, test, \"Machine_B_flag\")\n",
    "#print('Machine B flag Node ROC: ', machine_B_flag_roc)\n",
    "print('Machine B flag Node AUC: ', machine_B_flag_auc)\n",
    "\n",
    "# Model quality evaluation for failure Machine C\n",
    "# classification_report(bayesian_net, test, \"failure_Machine_C\")\n",
    "\n",
    "# ROC, AUC curve computation for failure Machine C\n",
    "machine_C_failure_roc, machine_C_failure_auc = roc_auc(bayesian_net, test, \"failure_Machine_C\")\n",
    "#print('Machine C failure node Node ROC: ', machine_C_failure_roc)\n",
    "print('Machine C failure node Node AUC: ', machine_C_failure_auc)\n",
    "\n",
    "# Model quality evaluation for Machine C flag\n",
    "# classification_report(bayesian_net, test, \"Machine_C_flag\")\n",
    "\n",
    "# ROC, AUC curve computation for Machine C flag\n",
    "machine_C_flag_roc, machine_C_flag_auc = roc_auc(bayesian_net, test, \"Machine_C_flag\")\n",
    "#print('Machine C flag Node ROC: ', machine_C_flag_roc)\n",
    "print('Machine C flag Node AUC: ', machine_C_flag_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxTJZxpkZCoc",
    "outputId": "8eacb796-023c-4b8c-febb-1a96cc4a367f"
   },
   "outputs": [],
   "source": [
    "# Model quality evaluation for failure Machine A\n",
    "classification_report(bayesian_net, test, \"failure_Machine_A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RO9IphkRahyg",
    "outputId": "714d17a3-2721-4191-8453-b3bd15d74e83"
   },
   "outputs": [],
   "source": [
    "# Model quality evaluation for Machine A flag\n",
    "classification_report(bayesian_net, test, \"Machine_A_flag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpURxpovBBFG",
    "outputId": "33df5095-d1d5-4057-b7bb-1f0138a112da"
   },
   "outputs": [],
   "source": [
    "# Model quality evaluation for failure Machine B\n",
    "classification_report(bayesian_net, test, \"failure_Machine_B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drtuoovoBDYc",
    "outputId": "c9aea4c2-98b0-4f3b-cdbc-bcf0acca9470"
   },
   "outputs": [],
   "source": [
    "# Model quality evaluation for Machine B flag\n",
    "classification_report(bayesian_net, test, \"Machine_B_flag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvVozLO1BGaI",
    "outputId": "245158bc-066e-4ec4-857d-baa12e56fef1"
   },
   "outputs": [],
   "source": [
    "# Model quality evaluation for failure Machine C\n",
    "classification_report(bayesian_net, test, \"failure_Machine_C\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inyrsidEBnEA",
    "outputId": "b28e08fb-c1d2-4a20-aded-faedf5c7760c"
   },
   "outputs": [],
   "source": [
    "# Model quality evaluation for Machine C flag\n",
    "classification_report(bayesian_net, test, \"Machine_C_flag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laBaw7vqFMu6"
   },
   "outputs": [],
   "source": [
    "# Fitting with the whole dataset\n",
    "bayesian_net = bayesian_net.fit_cpds(data, method=\"BayesianEstimator\", bayes_prior=\"K2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOyh9AwNG_kN",
    "outputId": "b3f8e06d-abca-463d-eee7-2c7bfccdb60b"
   },
   "outputs": [],
   "source": [
    "from causalnex.inference import InferenceEngine\n",
    "\n",
    "ie = InferenceEngine(bayesian_net)\n",
    "marginals = ie.query()\n",
    "marginals[\"Machine_C_flag\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmykFQlWDTGI",
    "outputId": "42c223b9-8984-421d-f3ba-6e1533c2eabf"
   },
   "outputs": [],
   "source": [
    "# Calculating conditioned probability Machine A\n",
    "p_break_given_flag_A = ie.query({\"Machine_A_flag\": 1})\n",
    "print(\"Marginal Failure Machine A | Machine A flag\", p_break_given_flag_A[\"failure_Machine_A\"])\n",
    "\n",
    "# Calculating conditioned probability Machine B\n",
    "p_break_given_flag_B = ie.query({\"Machine_B_flag\": 1})\n",
    "print(\"Marginal Failure Machine B | Machine B flag\", p_break_given_flag_B[\"failure_Machine_B\"])\n",
    "\n",
    "# Calculating conditioned probability Machine C\n",
    "p_break_given_flag_C = ie.query({\"Machine_C_flag\": 1})\n",
    "print(\"Marginal Failure Machine C | Machine C flag\", p_break_given_flag_C[\"failure_Machine_C\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MflPXwyHaZG",
    "outputId": "942194f3-dee7-479d-a6cd-1d0367b30da9"
   },
   "outputs": [],
   "source": [
    "# Calculating conditioned probability Machine C given failure A\n",
    "p_break_A_given_flag_C = ie.query({\"Machine_C_flag\": 1})\n",
    "print(\"Marginal Failure Machine A | Machine C flag\", p_break_A_given_flag_C[\"failure_Machine_A\"])\n",
    "\n",
    "# Calculating conditioned probability Machine C given failure B\n",
    "p_break_B_given_flag_C = ie.query({\"Machine_C_flag\": 1})\n",
    "print(\"Marginal Failure Machine B | Machine C flag\", p_break_B_given_flag_C[\"failure_Machine_B\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYtGw6k0H7Tx",
    "outputId": "b202e95d-13b7-496b-fc56-f715950fdd3d"
   },
   "outputs": [],
   "source": [
    "# Calculating conditioned probability Machine C given failure A\n",
    "p_flag_A_given_flag_C = ie.query({\"Machine_C_flag\": 1})\n",
    "print(\"Marginal A flag | Machine C flag\", p_break_A_given_flag_C[\"Machine_A_flag\"])\n",
    "\n",
    "# Calculating conditioned probability Machine C given failure B\n",
    "p_flag_B_given_flag_C = ie.query({\"Machine_C_flag\": 1})\n",
    "print(\"Marginal B flag | Machine C flag\", p_break_B_given_flag_C[\"Machine_B_flag\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sk02e0YSSCQm",
    "outputId": "e07f3b47-8a3e-474d-b8bc-8d1ff05008a0"
   },
   "outputs": [],
   "source": [
    "# Do calculus - Failure A vs Flag A\n",
    "print('distribution before do failure A', ie.query()['Machine_A_flag'])\n",
    "ie.do_intervention('failure_Machine_A', {1: 1.0, 0: 0.0})\n",
    "print('distribution after do failure A', ie.query()['Machine_A_flag'])\n",
    "ie.reset_do('failure_Machine_A')\n",
    "print('\\n')\n",
    "\n",
    "# Do calculus - Failure B vs Flag B\n",
    "print('distribution before do failure B', ie.query()['Machine_B_flag'])\n",
    "ie.do_intervention('failure_Machine_B', {1: 1.0, 0: 0.0})\n",
    "print('distribution after do failure B', ie.query()['Machine_B_flag'])\n",
    "ie.reset_do('failure_Machine_B')\n",
    "print('\\n')\n",
    "\n",
    "# Do calculus - Failure C vs Flag C\n",
    "print('distribution before do failure C', ie.query()['Machine_C_flag'])\n",
    "ie.do_intervention('failure_Machine_C', {1: 1.0, 0: 0.0})\n",
    "print('distribution after do failure C', ie.query()['Machine_C_flag'])\n",
    "ie.reset_do('failure_Machine_C')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-sA5yj3VTlg",
    "outputId": "2762f16f-5b3b-4c5f-bebe-f9bab9a23074"
   },
   "outputs": [],
   "source": [
    "# Do calculus - Failure A vs Flag C\n",
    "print('distribution before do failure A on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.do_intervention('failure_Machine_A', {1: 1.0, 0: 0.0})\n",
    "print('distribution after do failure A on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.reset_do('failure_Machine_A')\n",
    "print('\\n')\n",
    "\n",
    "# Do calculus - Failure B vs Flag C\n",
    "print('distribution before do failure B on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.do_intervention('failure_Machine_B', {1: 1.0, 0: 0.0})\n",
    "print('distribution after do failure B on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.reset_do('failure_Machine_B')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNSWQGLBJRGy",
    "outputId": "450853ca-d5c2-491c-cb3a-e3025443a26a"
   },
   "outputs": [],
   "source": [
    "# Do calculus - Flag A vs Flag C\n",
    "print('distribution before do flag A on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.do_intervention('Machine_A_flag', {1: 1.0, 0: 0.0})\n",
    "print('distribution after do flag A on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.reset_do('Machine_A_flag')\n",
    "print('\\n')\n",
    "\n",
    "# Do calculus - Flag B vs Flag C\n",
    "print('distribution before do flag B on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.do_intervention('Machine_B_flag', {1: 1.0, 0: 0.0})\n",
    "print('distribution after do flag B on flag C', ie.query()['Machine_C_flag'])\n",
    "ie.reset_do('Machine_B_flag')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNggPUafJuOk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNHTMbXzjCzzhmy58YfwFsp",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1ttKTicFZiOkWLE0ijLhWxAEMsnRce2HN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
